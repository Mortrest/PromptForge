{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Set, Optional\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    \"\"\"\n",
    "    Generate diverse but relevant stories about a brave knight using \n",
    "    a combination of hard and soft prompts.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"gpt2-medium\",\n",
    "        soft_prompt_length: int = 10,\n",
    "        learning_rate: float = 0.01,\n",
    "        batch_size: int = 10,\n",
    "        max_length: int = 200,\n",
    "        diversity_weight: float = 0.5,\n",
    "        num_reference_stories: int = 20,\n",
    "        update_frequency: int = 5,\n",
    "        recent_stories_window: int = 20,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Knight Story Generator.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the pretrained language model\n",
    "            soft_prompt_length: Length of the soft prompt embedding\n",
    "            learning_rate: Learning rate for soft prompt optimization\n",
    "            batch_size: Number of candidate stories to generate per iteration\n",
    "            max_length: Maximum length of generated stories\n",
    "            diversity_weight: Weight (Î») for balancing relevance and diversity\n",
    "            num_reference_stories: Number of reference stories to generate initially\n",
    "            update_frequency: How often to update the soft prompt (in stories)\n",
    "            recent_stories_window: Number of recent stories to consider for diversity\n",
    "            device: Device to run computations on\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.soft_prompt_length = soft_prompt_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.num_reference_stories = num_reference_stories\n",
    "        self.update_frequency = update_frequency\n",
    "        self.recent_stories_window = recent_stories_window\n",
    "        self.device = device\n",
    "        self.hard_prompt = \"Write a story about a brave knight.\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Set pad token for GPT models that don't have it\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "        print(f\"Model embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "        # Initialize soft prompt\n",
    "        self.initialize_soft_prompt()\n",
    "        \n",
    "        # Initialize storage for reference and selected stories\n",
    "        self.reference_stories = []\n",
    "        self.reference_embeddings = []\n",
    "        self.selected_stories = []\n",
    "        self.selected_embeddings = []\n",
    "    \n",
    "    def initialize_soft_prompt(self):\n",
    "        \"\"\"\n",
    "        Initialize the soft prompt with embeddings from relevant words\n",
    "        instead of random values.\n",
    "        \"\"\"\n",
    "        # 1. Meaningful initialization with relevant knight-related words\n",
    "        knight_words = [\n",
    "            \"knight\", \"brave\", \"hero\", \"sword\", \"quest\", \"honor\", \"valor\",\n",
    "            \"adventure\", \"battle\", \"courage\", \"noble\", \"chivalry\", \"journey\",\n",
    "            \"kingdom\", \"dragon\", \"rescue\", \"shield\", \"armor\", \"gallant\", \"loyal\"\n",
    "        ]\n",
    "        \n",
    "        # Ensure we have enough words (repeat if necessary)\n",
    "        if len(knight_words) < self.soft_prompt_length:\n",
    "            knight_words = knight_words * (self.soft_prompt_length // len(knight_words) + 1)\n",
    "        \n",
    "        # Select subset if we have too many\n",
    "        knight_words = knight_words[:self.soft_prompt_length]\n",
    "        \n",
    "        print(f\"Initializing soft prompt with words: {knight_words}\")\n",
    "        \n",
    "        # Get word embeddings from model vocabulary\n",
    "        soft_prompt_embeds = []\n",
    "        \n",
    "        for word in knight_words:\n",
    "            # Get token ID for the word\n",
    "            token_id = self.tokenizer.encode(\" \" + word, add_special_tokens=False)[0]\n",
    "            \n",
    "            # Get embedding from model\n",
    "            with torch.no_grad():\n",
    "                word_embedding = self.model.transformer.wte.weight[token_id].clone()\n",
    "            \n",
    "            soft_prompt_embeds.append(word_embedding)\n",
    "        \n",
    "        # Stack embeddings\n",
    "        self.soft_prompt = torch.stack(soft_prompt_embeds, dim=0).to(self.device)\n",
    "        self.soft_prompt.requires_grad = True\n",
    "        \n",
    "        # Store these initial embeddings for regularization\n",
    "        self.initial_embeddings = self.soft_prompt.clone().detach()\n",
    "        \n",
    "        # 3. Careful updates - use smaller learning rate\n",
    "        self.optimizer = optim.Adam([self.soft_prompt], lr=self.learning_rate * 0.5)\n",
    "        \n",
    "        # 4. Anchor words (we'll keep some of the same words, but add some with stronger emotional/narrative content)\n",
    "        self.anchor_words = [\n",
    "            \"honor\", \"adventure\", \"courage\", \"quest\", \"legend\", \"triumph\",\n",
    "            \"journey\", \"battle\", \"hero\", \"destiny\"\n",
    "        ]\n",
    "        \n",
    "        # Get embeddings for anchor words\n",
    "        self.anchor_embeddings = []\n",
    "        for word in self.anchor_words:\n",
    "            token_id = self.tokenizer.encode(\" \" + word, add_special_tokens=False)[0]\n",
    "            with torch.no_grad():\n",
    "                anchor_embedding = self.model.transformer.wte.weight[token_id].clone()\n",
    "            self.anchor_embeddings.append(anchor_embedding.to(self.device))\n",
    "        \n",
    "        print(f\"Initialized soft prompt with {len(knight_words)} word embeddings\")\n",
    "    \n",
    "    def generate_reference_stories(self):\n",
    "        \"\"\"\n",
    "        Generate initial reference stories using just the hard prompt.\n",
    "        \"\"\"\n",
    "        print(f\"Generating {self.num_reference_stories} reference stories...\")\n",
    "        \n",
    "        # Tokenize the hard prompt\n",
    "        inputs = self.tokenizer(self.hard_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Generate reference stories\n",
    "        for i in range(self.num_reference_stories):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=self.max_length,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    top_k=50,\n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "                \n",
    "                # Decode the generated story\n",
    "                story = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Store the story\n",
    "                self.reference_stories.append(story)\n",
    "                \n",
    "                # Compute and store the story embedding\n",
    "                embedding = self.get_story_embedding(story)\n",
    "                self.reference_embeddings.append(embedding)\n",
    "                \n",
    "                print(f\"Reference story {i+1}: {story[:100]}...\")\n",
    "    \n",
    "    def get_story_embedding(self, story: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute an embedding for a story by averaging token embeddings.\n",
    "        \n",
    "        Args:\n",
    "            story: The text of the story\n",
    "            \n",
    "        Returns:\n",
    "            Embedding tensor\n",
    "        \"\"\"\n",
    "        # Tokenize the story\n",
    "        tokens = self.tokenizer(story, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Get embeddings for tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                tokens.input_ids,\n",
    "                attention_mask=tokens.attention_mask,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Use the hidden states from the last layer\n",
    "            last_hidden_states = outputs.hidden_states[-1]\n",
    "            \n",
    "            # Average over tokens to get story embedding\n",
    "            # Use attention mask to ignore padding tokens\n",
    "            mask = tokens.attention_mask.unsqueeze(-1)\n",
    "            embedding = (last_hidden_states * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "            \n",
    "            return embedding.squeeze(0)\n",
    "    \n",
    "    def generate_candidate_stories(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate candidate stories using the soft prompt + hard prompt.\n",
    "        \n",
    "        Returns:\n",
    "            List of candidate stories\n",
    "        \"\"\"\n",
    "        # Tokenize hard prompt\n",
    "        hard_prompt_ids = self.tokenizer.encode(self.hard_prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Get model embeddings for hard prompt\n",
    "        with torch.no_grad():\n",
    "            hard_prompt_embeds = self.model.transformer.wte(hard_prompt_ids)\n",
    "            \n",
    "        # Concat soft prompt and hard prompt embeddings\n",
    "        input_embeds = torch.cat([self.soft_prompt.unsqueeze(0), hard_prompt_embeds], dim=1)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones(1, input_embeds.size(1), dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Generate candidate stories\n",
    "        candidates = []\n",
    "        \n",
    "        for _ in range(self.batch_size):\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        inputs_embeds=input_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=input_embeds.size(1) + self.max_length,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50,\n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=0.8,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        num_return_sequences=1,\n",
    "                        return_dict_in_generate=True\n",
    "                    )\n",
    "                    \n",
    "                    # Extract the generated token IDs (skip the initial input tokens)\n",
    "                    generated_ids = outputs.sequences[0][input_embeds.size(1):]\n",
    "                    \n",
    "                    # Decode to text\n",
    "                    story = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "                    \n",
    "                    if story.strip():  # Only add non-empty texts\n",
    "                        candidates.append(story)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating candidate: {e}\")\n",
    "                \n",
    "        return candidates\n",
    "    \n",
    "    def compute_relevance_scores(self, candidates: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute relevance scores for candidates based on log probability.\n",
    "        \n",
    "        Args:\n",
    "            candidates: List of candidate stories\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of relevance scores\n",
    "        \"\"\"\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for story in candidates:\n",
    "            try:\n",
    "                # Tokenize the story\n",
    "                tokens = self.tokenizer(story, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # Compute log probability\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(\n",
    "                        tokens.input_ids,\n",
    "                        attention_mask=tokens.attention_mask,\n",
    "                        labels=tokens.input_ids,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    \n",
    "                    # Lower loss means higher probability\n",
    "                    log_prob = -outputs.loss.item()\n",
    "                    relevance_scores.append(log_prob)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing relevance: {e}\")\n",
    "                relevance_scores.append(-float('inf'))  # Assign worst score\n",
    "                \n",
    "        return torch.tensor(relevance_scores, device=self.device)\n",
    "    \n",
    "    def compute_diversity_scores(self, candidates: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute diversity scores for candidates based on embeddings.\n",
    "        \n",
    "        Args:\n",
    "            candidates: List of candidate stories\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of diversity scores\n",
    "        \"\"\"\n",
    "        # If no stories selected yet, all candidates are maximally diverse\n",
    "        if not self.selected_embeddings:\n",
    "            return torch.ones(len(candidates), device=self.device)\n",
    "        \n",
    "        # Get recent story embeddings for diversity calculation\n",
    "        recent_embeddings = self.selected_embeddings[-self.recent_stories_window:]\n",
    "        \n",
    "        diversity_scores = []\n",
    "        \n",
    "        for story in candidates:\n",
    "            try:\n",
    "                # Compute story embedding\n",
    "                embedding = self.get_story_embedding(story)\n",
    "                \n",
    "                # Compute similarity to all recent selected stories\n",
    "                similarities = torch.stack([\n",
    "                    F.cosine_similarity(embedding, selected_emb, dim=0) \n",
    "                    for selected_emb in recent_embeddings\n",
    "                ])\n",
    "                \n",
    "                # Diversity is negative of maximum similarity\n",
    "                # (lower similarity to existing stories = higher diversity)\n",
    "                max_similarity = torch.max(similarities)\n",
    "                diversity = -max_similarity.item()\n",
    "                \n",
    "                diversity_scores.append(diversity)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error computing diversity: {e}\")\n",
    "                diversity_scores.append(-float('inf'))  # Assign worst score\n",
    "                \n",
    "        return torch.tensor(diversity_scores, device=self.device)\n",
    "    \n",
    "    def update_soft_prompt(self):\n",
    "        \"\"\"\n",
    "        Update the soft prompt using contrastive loss to balance\n",
    "        relevance and diversity, with added regularization and anchoring.\n",
    "        \"\"\"\n",
    "        print(\"Updating soft prompt...\")\n",
    "        \n",
    "        # Skip if we don't have both reference and selected stories\n",
    "        if not self.reference_stories or not self.selected_stories:\n",
    "            print(\"Not enough stories to update soft prompt.\")\n",
    "            return\n",
    "        \n",
    "        # Use recent selected stories for efficiency\n",
    "        recent_selected = self.selected_stories[-self.recent_stories_window:]\n",
    "        \n",
    "        # Hyperparameters for regularization and anchoring\n",
    "        reg_weight = 0.1  # Weight for regularization term\n",
    "        anchor_weight = 0.2  # Weight for anchor term\n",
    "        \n",
    "        # Contrastive loss optimization\n",
    "        for _ in range(5):  # Update for 5 steps each time\n",
    "            \n",
    "            # Compute loss for reference stories (we want high probability for these)\n",
    "            ref_loss = 0.0\n",
    "            for story in self.reference_stories:\n",
    "                # Get hard prompt embeddings\n",
    "                hard_prompt_ids = self.tokenizer.encode(self.hard_prompt, return_tensors=\"pt\").to(self.device)\n",
    "                hard_prompt_embeds = self.model.transformer.wte(hard_prompt_ids)\n",
    "                \n",
    "                # Combine with soft prompt\n",
    "                input_embeds = torch.cat([self.soft_prompt.unsqueeze(0), hard_prompt_embeds], dim=1)\n",
    "                \n",
    "                # Create attention mask\n",
    "                attention_mask = torch.ones(1, input_embeds.size(1), dtype=torch.long, device=self.device)\n",
    "                \n",
    "                # Run the model to get the logits for the prompt\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(\n",
    "                        inputs_embeds=input_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    prompt_logits = outputs.logits[:, -1, :]  # Take logits from last position\n",
    "                \n",
    "                # Get story tokens and compute their likelihood given the prompt\n",
    "                story_tokens = self.tokenizer.encode(story, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # For first few tokens, compute logprob\n",
    "                story_logprob = 0.0\n",
    "                for i in range(min(5, story_tokens.size(1))):  # Look at first 5 tokens max\n",
    "                    token_id = story_tokens[0, i].item()\n",
    "                    token_logprob = F.log_softmax(prompt_logits, dim=-1)[0, token_id]\n",
    "                    story_logprob += token_logprob\n",
    "                \n",
    "                # Add negative logprob to loss (we want to maximize probability)\n",
    "                ref_loss -= story_logprob\n",
    "            \n",
    "            # Compute loss for selected stories (we want low probability for these)\n",
    "            sel_loss = 0.0\n",
    "            for story in recent_selected:\n",
    "                # Get hard prompt embeddings\n",
    "                hard_prompt_ids = self.tokenizer.encode(self.hard_prompt, return_tensors=\"pt\").to(self.device)\n",
    "                hard_prompt_embeds = self.model.transformer.wte(hard_prompt_ids)\n",
    "                \n",
    "                # Combine with soft prompt\n",
    "                input_embeds = torch.cat([self.soft_prompt.unsqueeze(0), hard_prompt_embeds], dim=1)\n",
    "                \n",
    "                # Create attention mask\n",
    "                attention_mask = torch.ones(1, input_embeds.size(1), dtype=torch.long, device=self.device)\n",
    "                \n",
    "                # Run the model to get the logits for the prompt\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(\n",
    "                        inputs_embeds=input_embeds,\n",
    "                        attention_mask=attention_mask,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    prompt_logits = outputs.logits[:, -1, :]  # Take logits from last position\n",
    "                \n",
    "                # Get story tokens and compute their likelihood given the prompt\n",
    "                story_tokens = self.tokenizer.encode(story, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                # For first few tokens, compute logprob\n",
    "                story_logprob = 0.0\n",
    "                for i in range(min(5, story_tokens.size(1))):  # Look at first 5 tokens max\n",
    "                    token_id = story_tokens[0, i].item()\n",
    "                    token_logprob = F.log_softmax(prompt_logits, dim=-1)[0, token_id]\n",
    "                    story_logprob += token_logprob\n",
    "                \n",
    "                # Add logprob to loss (we want to minimize probability)\n",
    "                sel_loss += story_logprob\n",
    "            \n",
    "            # 2. Regularization: Add term to keep soft prompt close to real word embeddings\n",
    "            # Compute distance to initial embeddings (which are real word embeddings)\n",
    "            reg_loss = torch.norm(self.soft_prompt - self.initial_embeddings, p=2)\n",
    "            \n",
    "            # # 4. Anchoring: Keep soft prompt close to key anchor words\n",
    "            # anchor_loss = 0.0\n",
    "            # for i, anchor_emb in enumerate(self.anchor_embeddings):\n",
    "            #     # Find the closest soft prompt vector to this anchor\n",
    "            #     distances = torch.sum((self.soft_prompt.unsqueeze(1) - anchor_emb.unsqueeze(0))**2, dim=2)\n",
    "            #     min_dist_idx = torch.argmin(distances)\n",
    "                \n",
    "            #     # Add loss term to pull this vector closer to the anchor\n",
    "            #     anchor_loss += torch.norm(self.soft_prompt[min_dist_idx] - anchor_emb, p=2)\n",
    "            \n",
    "            # # Scale the anchor loss by number of anchors\n",
    "            # if self.anchor_embeddings:\n",
    "            #     anchor_loss /= len(self.anchor_embeddings)\n",
    "            \n",
    "            # Total loss combines all objectives\n",
    "            total_loss = ref_loss + sel_loss + reg_weight * reg_loss \n",
    "            \n",
    "            # Update soft prompt\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # 3. Careful updates: Apply gradient clipping to prevent large updates\n",
    "            torch.nn.utils.clip_grad_norm_([self.soft_prompt], max_norm=1.0)\n",
    "            \n",
    "            # Perform update\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            print(f\"Soft prompt update step, loss: {total_loss.item():.4f} (ref: {ref_loss.item():.4f}, \"\n",
    "                       f\"sel: {sel_loss.item():.4f}, reg: {reg_loss.item():.4f})\")\n",
    "    \n",
    "    def generate_stories(self, num_stories: int = 100, output_file: str = \"knight_stories.txt\"):\n",
    "        \"\"\"\n",
    "        Generate a collection of diverse but relevant knight stories.\n",
    "        \n",
    "        Args:\n",
    "            num_stories: Number of stories to generate\n",
    "            output_file: File to save the stories\n",
    "        \"\"\"\n",
    "        print(f\"Generating {num_stories} knight stories...\")\n",
    "        \n",
    "        # Step 1: Generate reference stories with just the hard prompt\n",
    "        self.generate_reference_stories()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress_bar = tqdm.tqdm(range(num_stories), desc=\"Generating stories\")\n",
    "        \n",
    "        # Step 2: Main generation loop\n",
    "        for i in progress_bar:\n",
    "            # Generate candidate stories\n",
    "            candidates = self.generate_candidate_stories()\n",
    "            \n",
    "            if not candidates:\n",
    "                print(\"No valid candidates generated, retrying...\")\n",
    "                continue\n",
    "            \n",
    "            # Score candidates for relevance and diversity\n",
    "            relevance_scores = self.compute_relevance_scores(candidates)\n",
    "            diversity_scores = self.compute_diversity_scores(candidates)\n",
    "            \n",
    "            # Combine scores\n",
    "            combined_scores = relevance_scores + self.diversity_weight * diversity_scores\n",
    "            \n",
    "            # Find best candidate\n",
    "            best_idx = torch.argmax(combined_scores).item()\n",
    "            best_story = candidates[best_idx]\n",
    "            \n",
    "            # Add to selected stories\n",
    "            self.selected_stories.append(best_story)\n",
    "            \n",
    "            # Compute and store embedding\n",
    "            best_embedding = self.get_story_embedding(best_story)\n",
    "            self.selected_embeddings.append(best_embedding)\n",
    "            \n",
    "            # Log progress\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Generated story {i+1}: {best_story[:100]}...\")\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                \"Relevance\": f\"{relevance_scores[best_idx]:.4f}\",\n",
    "                \"Diversity\": f\"{diversity_scores[best_idx]:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Step 3: Update soft prompt periodically\n",
    "            if (i + 1) % self.update_frequency == 0:\n",
    "                self.update_soft_prompt()\n",
    "        \n",
    "        print(f\"Saving {len(self.selected_stories)} stories to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for i, story in enumerate(self.selected_stories):\n",
    "                    f.write(f\"=== Story {i+1} ===\\n\")\n",
    "                    f.write(story + '\\n\\n')\n",
    "                    \n",
    "            print(f\"Successfully saved stories to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving stories: {e}\")\n",
    "\n",
    "def find_closest_word_in_vocabulary(model, tokenizer, embedding, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the closest word in vocabulary to the given embedding.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        embedding: Embedding vector to match\n",
    "        top_k: Number of closest matches to return\n",
    "        \n",
    "    Returns:\n",
    "        List of closest word matches\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        embedding_matrix = model.transformer.wte.weight\n",
    "        \n",
    "        embedding = embedding.to(embedding_matrix.device)\n",
    "        cos_similarities = F.cosine_similarity(embedding.unsqueeze(0), embedding_matrix, dim=1)\n",
    "        \n",
    "        top_indices = torch.argsort(cos_similarities, descending=True)[:top_k].tolist()\n",
    "        \n",
    "        closest_words = []\n",
    "        for idx in top_indices:\n",
    "            token = tokenizer.decode([idx]).strip()\n",
    "            closest_words.append(token)\n",
    "        \n",
    "        return closest_words\n",
    "\n",
    "def visualize_soft_prompt(generator):\n",
    "    \"\"\"\n",
    "    Visualize the soft prompt by finding the closest words in vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        generator: KnightStoryGenerator instance\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Soft Prompt Visualization ===\")\n",
    "    \n",
    "    for i, embedding in enumerate(generator.soft_prompt):\n",
    "        closest_words = find_closest_word_in_vocabulary(\n",
    "            generator.model, \n",
    "            generator.tokenizer, \n",
    "            embedding\n",
    "        )\n",
    "        \n",
    "        print(f\"Position {i+1}: {', '.join(closest_words)}\")\n",
    "    \n",
    "    print(\"\\n=== Initial Anchor Words ===\")\n",
    "    for i, word in enumerate(generator.anchor_words):\n",
    "        print(f\"Anchor {i+1}: {word}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        generator = Generator(\n",
    "            model_name=\"gpt2-medium\",\n",
    "            soft_prompt_length=5,  # Slightly shorter for more focused prompts\n",
    "            batch_size=10,\n",
    "            max_length=300,\n",
    "            diversity_weight=0.7,\n",
    "            num_reference_stories=20,\n",
    "            update_frequency=5,\n",
    "            learning_rate=0.005  # Reduced learning rate for careful updates\n",
    "        )\n",
    "        \n",
    "        visualize_soft_prompt(generator)\n",
    "        \n",
    "        generator.generate_stories(\n",
    "            num_stories=50,  # Reduced for testing\n",
    "            output_file=\"outputs/knight_stories.txt\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Final Soft Prompt ===\")\n",
    "        visualize_soft_prompt(generator)\n",
    "        \n",
    "        print(\"\\nSuccessfully generated knight stories!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error running Knight Story Generator: {e}\")\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
